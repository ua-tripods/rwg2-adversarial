@article{Poupon2004,
title = "Voronoi and Voronoi-related tessellations in studies of protein structure and interaction",
journal = "Current Opinion in Structural Biology",
volume = "14",
number = "2",
pages = "233 - 241",
year = "2004",
issn = "0959-440X",
doi = "https://doi.org/10.1016/j.sbi.2004.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S0959440X04000442",
author = "Anne Poupon"
}                  
                  

@article {Guo1997,
author = {Guo, Baining and Menon, Jai and Willette, Brian},
title = {Surface Reconstruction Using Alpha Shapes},
journal = {Computer Graphics Forum},
volume = {16},
number = {4},
publisher = {Blackwell Publishers},
issn = {1467-8659},
url = {http://dx.doi.org/10.1111/1467-8659.00178},
doi = {10.1111/1467-8659.00178},
pages = {177--190},
keywords = {Surface topology, alpha shapes, manifolds, surface fitting},
year = {1997},
}

@Article{Maus1984,
author="Maus, Arne",
title="Delaunay triangulation and the convex hull ofn points in expected linear time",
journal="BIT Numerical Mathematics",
year="1984",
month="Jun",
day="01",
volume="24",
number="2",
pages="151--163",
abstract="An algorithm is presented which produces a Delaunay triangulation ofn points in the Euclidean plane in expected linear time. The expected execution time is achieved when the data are (not too far from) uniformly distributed. A modification of the algorithm discussed in the appendix treats most of the non-uniform distributions. The basis of this algorithm is a geographical partitioning of the plane into boxes by the well-known Radix-sort algorithm. This partitioning is also used as a basis for a linear time algorithm for finding the convex hull ofn points in the Euclidean plane.",
issn="1572-9125",
doi="10.1007/BF01937482",
url="https://doi.org/10.1007/BF01937482"
}

@Article{Lee1980,
author="Lee, D. T.
and Schachter, B. J.",
title="Two algorithms for constructing a Delaunay triangulation",
journal="International Journal of Computer {\&} Information Sciences",
year="1980",
month="Jun",
day="01",
volume="9",
number="3",
pages="219--242",
abstract="This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.",
issn="1573-7640",
doi="10.1007/BF00977785",
url="https://doi.org/10.1007/BF00977785"
}

@ARTICLE{Edelsbrunner1983, 
author={H. Edelsbrunner and D. Kirkpatrick and R. Seidel}, 
journal={IEEE Transactions on Information Theory}, 
title={On the shape of a set of points in the plane}, 
year={1983}, 
volume={29}, 
number={4}, 
pages={551-559}, 
keywords={Geometry;Image analysis, shape;Image shape analysis}, 
doi={10.1109/TIT.1983.1056714}, 
ISSN={0018-9448}, 
month={July},}

                  % dynamical clustering paper
@article{Drieme2017,
  author    = {Anne Driemel and
               Francesco Silvestri},
  title     = {Locality-sensitive hashing of curves},
  journal   = {CoRR},
  volume    = {abs/1703.04040},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04040},
  archivePrefix = {arXiv},
  eprint    = {1703.04040},
  timestamp = {Tue, 18 Jul 2017 10:49:06 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/DriemelS17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Sankararaman2013,
 author = {Sankararaman, Swaminathan and Agarwal, Pankaj K. and M{\o}lhave, Thomas and Pan, Jiangwei and Boedihardjo, Arnold P.},
 title = {Model-driven Matching and Segmentation of Trajectories},
 booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
 series = {SIGSPATIAL'13},
 year = {2013},
 isbn = {978-1-4503-2521-9},
 location = {Orlando, Florida},
 pages = {234--243},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2525314.2525360},
 doi = {10.1145/2525314.2525360},
 acmid = {2525360},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPS trajectories, trajectory matching, trajectory segmentation},
} 


@ARTICLE{Mirzargar2014, 
author={M. Mirzargar and R. T. Whitaker and R. M. Kirby}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Curve Boxplot: Generalization of Boxplot for Ensembles of Curves}, 
year={2014}, 
volume={20}, 
number={12}, 
pages={2654-2663}, 
keywords={computational geometry;data visualisation;boundary values;boxplot generalization;computational scientists;curve boxplot;curve ensembles;data depth;descriptive statistics;nonparametric method;rendering strategies;simulation science;visualization community;visualization strategies;Computational modeling;Curve fitting;Data visualization;Robustness;Shape analysis;Statistical analysis;Uncertainty visualization;boxplots;data depth;ensemble visualization;functional data;nonparametric statistic;order statistics;parametric curves;0}, 
doi={10.1109/TVCG.2014.2346455}, 
ISSN={1077-2626}, 
month={Dec},}

@article{Raj2017,
title = "Path Boxplots: A Method for Characterizing Uncertainty in Path Ensembles on a Graph",
abstract = "Graphs are powerful and versatile data structures that can be used to represent a wide range of different types of information. In this article, we introduce a method to analyze and then visualize an important class of data described over a graph-namely, ensembles of paths. Analysis of such path ensembles is useful in a variety of applications, in diverse fields such as transportation, computer networks, and molecular dynamics. The proposed method generalizes the concept of band depth to an ensemble of paths on a graph, which provides a center-outward ordering on the paths. This ordering is, in turn, used to construct a generalization of the conventional boxplot or whisker plot, called a path boxplot, which applies to paths on a graph. The utility of path boxplot is demonstrated for several examples of path ensembles including paths defined over computer networks and roads. Supplementary materials for this article are available online.",
keywords = "Data depth, Descriptive statistics, Ensemble, Graph, Paths",
author = "Mukund Raj and Mahsa Mirzargar and Robert Ricci and Kirby, {Robert M.} and Whitaker, {Ross T.}",
year = "2017",
month = "4",
doi = "10.1080/10618600.2016.1209115",
volume = "26",
pages = "243--252",
journal = "Journal of Computational and Graphical Statistics",
issn = "1061-8600",
publisher = "American Statistical Association",
number = "2",
}
                  
@Inbook{Kharrat2008,
author="Kharrat, Ahmed
and Popa, Iulian Sandu
and Zeitouni, Karine
and Faiz, Sami",
nothing="Ruas, Anne
and Gold, Christopher",
title="Clustering Algorithm for Network Constraint Trajectories",
bookTitle="Headway in Spatial Data Handling: 13th International Symposium on Spatial Data Handling",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="631--647",
abstract="Spatial data mining is an active topic in spatial databases. This paper proposes a new clustering method for moving object trajectories databases. It applies specifically to trajectories that only lie on a predefined network. The proposed algorithm (NETSCAN) is inspired from the well-known density based algorithms. However, it takes advantage of the network constraint to estimate the object density. Indeed, NETSCAN first computes dense paths in the network based on the moving object count, then, it clusters the sub-trajectories which are similar to the dense paths. The user can adjust the clustering result by setting a density threshold for the dense paths, and a similarity threshold within the clusters. This paper describes the proposed method. An implementation is reported, along with experimental results that show the effectiveness of our approach and the flexibility allowed by the user parameters.",
isbn="978-3-540-68566-1",
doi="10.1007/978-3-540-68566-1_36",
url="https://doi.org/10.1007/978-3-540-68566-1_36"
}
                  
@article{Zheng2015,
 author = {Zheng, Yu},
 title = {Trajectory Data Mining: An Overview},
 journal = {ACM Trans. Intell. Syst. Technol.},
 issue_date = {May 2015},
 volume = {6},
 number = {3},
 month = {may},
 year = {2015},
 issn = {2157-6904},
 pages = {29:1--29:41},
 articleno = {29},
 numpages = {41},
 url = {http://doi.acm.org/10.1145/2743025},
 doi = {10.1145/2743025},
 acmid = {2743025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Spatiotemporal data mining, trajectory classification, trajectory compression, trajectory data mining, trajectory indexing and retrieval, trajectory outlier detection, trajectory pattern mining, trajectory uncertainty, urban computing},
} 

@article{Shah12017,
author = {Shah, Sohil Atul and Koltun, Vladlen}, 
title = {Robust continuous clustering},
volume = {114}, 
number = {37}, 
pages = {9814-9819}, 
year = {2017}, 
doi = {10.1073/pnas.1700770114}, 
abstract ={Clustering is a fundamental procedure in the analysis of scientific data. It is used ubiquitously across the sciences. Despite decades of research, existing clustering algorithms have limited effectiveness in high dimensions and often require tuning parameters for different domains and datasets. We present a clustering algorithm that achieves high accuracy across multiple domains and scales efficiently to high dimensions and large datasets. The presented algorithm optimizes a smooth continuous objective, which is based on robust statistics and allows heavily mixed clusters to be untangled. The continuous nature of the objective also allows clustering to be integrated as a module in end-to-end feature learning pipelines. We demonstrate this by extending the algorithm to perform joint clustering and dimensionality reduction by efficiently optimizing a continuous global objective. The presented approach is evaluated on large datasets of faces, hand-written digits, objects, newswire articles, sensor readings from the Space Shuttle, and protein expression levels. Our method achieves high accuracy across all datasets, outperforming the best prior algorithm by a factor of 3 in average rank.}, 
URL = {http://www.pnas.org/content/114/37/9814.abstract}, 
eprint = {http://www.pnas.org/content/114/37/9814.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@ARTICLE{Jaromczyk1982, 
author={J. W. Jaromczyk and G. T. Toussaint}, 
journal={Proceedings of the IEEE}, 
title={Relative neighborhood graphs and their relatives}, 
year=1992, 
volume=80, 
number=9, 
pages={1502-1517}, 
keywords={computational geometry;computer vision;pattern recognition;spatial data structures;visual databases;computational morphology;computer vision;databases;neighborhood graphs;pattern classification;spatial analysis;Application software;Bibliographies;Biology computing;Computational geometry;Computer applications;Computer science;Computer vision;Morphology;Pattern analysis;Shape}, 
doi={10.1109/5.163414}, 
ISSN={0018-9219}, 
month={Sep},}

@inproceedings{Chakrabarti2006,
  title={Evolutionary clustering},
  author={Chakrabarti, Deepayan and Kumar, Ravi and Tomkins, Andrew},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining}, 
  pages={554--560},
  year={2006},
  organization={ACM}
}

@inproceedings{Andrade2001,
  title={Good approximations for the relative neighbourhood graph.},
  author={Andrade, Diogo Vieira and de Figueiredo, Luiz Henrique},
  booktitle={CCCG},
  pages={25--28},
  year={2001}
}

@book{greene2003,
  title={Econometric Analysis},
  author={Greene, W.H.},
  isbn={9788177586848},
  url={https://books.google.com/books?id=njAcXDlR5U8C},
  year={2003},
  publisher={Pearson Education}
}
                  
@article{mcfadden1973,
  title={Conditional logit analysis of qualitative choice behavior},
  author={McFadden, Daniel and others},
  year={1973},
  journal={Frontiers in Econometrics},
  publisher={Institute of Urban and Regional Development, University of California}
}

@phdthesis{novelli2015,
  title={Detection and Measurement of Sales Cannibalization in Information Technology Markets},
  author={Novelli, Francesco},
  year={2015},
  school={Technische Universit{\"a}t}
}

@article{draganska2004,
  title={A likelihood approach to estimating market equilibrium models},
  author={Draganska, Michaela and Jain, Dipak},
  journal={Management Science},
  volume={50},
  number={5},
  pages={605--616},
  year={2004},
  publisher={INFORMS}
}

@MISC{shriver2015,
  title={A Structural Model of Channel Choice with Implications for Retail Entry},
  author={Shriver, Scott and Bollinger, Bryan},
  year=2015
}

@article{fisher2009,
  title={An algorithm and demand estimation procedure for retail assortment optimization},
  author={Fisher, Marshall L and Vaidyanathan, Ramnath},
  journal={Philadelphia: The Wharton School},
  year={2009}
}

@article{liu1990,
  title={On a notion of data depth based on random simplices},
  author={Liu, Regina Y and others},
  journal={The Annals of Statistics},
  volume={18},
  number={1},
  pages={405--414},
  year={1990},
  publisher={Institute of Mathematical Statistics}
}

@article{lopez2009,
  title={On the concept of depth for functional data},
  author={L{\'o}pez-Pintado, Sara and Romo, Juan},
  journal={Journal of the American Statistical Association},
  volume={104},
  number={486},
  pages={718--734},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{rousseeuw1996,
  title={Algorithm AS 307: Bivariate location depth},
  author={Rousseeuw, Peter J and Ruts, Ida},
  journal={Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume={45},
  number={4},
  pages={516--526},
  year={1996},
  publisher={JSTOR}
}                  
                  
@inproceedings{cheng2001,
  title={On algorithms for simplicial depth.},
  author={Cheng, Andrew Y and Ouyang, Ming},
  booktitle={CCCG},
  pages={53--56},
  year={2001}
}
                  
@article{krishnan2006,
  title={Statistical data depth and the graphics hardware},
  author={Krishnan, Suresh and Mustafa, Nabil H and Venkatasubramanian, Suresh},
  journal={DIMACS Series in Discrete Mathematics and Theoretical Computer Science},
  volume={72},
  pages={223},
  year={2006},
  publisher={AMERICAN MATHEMATICAL SOCIETY}
}

@article{Zasenko2016,
  author    = {Olga Zasenko and
               Tamon Stephen},
  title     = {Algorithms for Colourful Simplicial Depth and Medians in the Plane},
  journal   = {CoRR},
  volume    = {abs/1608.07348},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.07348},
  archivePrefix = {arXiv},
  eprint    = {1608.07348},
  timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/ZasenkoS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}                  
                  
@article{Linderman2017,
  author    = {George C. Linderman and
               Stefan Steinerberger},
  title     = {Clustering with t-SNE, provably},
  journal   = {CoRR},
  volume    = {abs/1706.02582},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02582},
  archivePrefix = {arXiv},
  eprint    = {1706.02582},
  timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/LindermanS17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{wattenberg2016how,
  author = {Wattenberg, Martin and Viegas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

@INPROCEEDINGS{Lee_Verleysen2014, 
author={J. A. Lee and M. Verleysen}, 
booktitle={2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}, 
title={Two key properties of dimensionality reduction methods}, 
year={2014}, 
volume={}, 
number={}, 
pages={163-170}, 
keywords={data reduction;data structures;neural nets;principal component analysis;DR;data representation;deep neural networks;dimensionality reduction;principal component analysis;Cost function;Covariance matrices;Force;Manifolds;Plastics;Principal component analysis;Vectors}, 
doi={10.1109/CIDM.2014.7008663}, 
ISSN={}, 
month={Dec},}

@inproceedings{simon1996,
  title={Anonymous communication and anonymous cash},
  author={Simon, Daniel R},
  booktitle={Annual International Cryptology Conference},
  pages={61--73},
  year={1996},
  organization={Springer}
}
@inproceedings{al2016,
  title={Categorical Compositional Cognition},
  author={Al-Mehairi, Yaared and Coecke, Bob and Lewis, Martha},
  booktitle={International Symposium on Quantum Interaction},
  pages={122--134},
  year={2016},
  organization={Springer}
}
                  
@article{kartsaklis2013,
  title={Reasoning about meaning in natural language with compact closed categories and frobenius algebras},
  author={Kartsaklis, Dimitri and Sadrzadeh, Mehrnoosh and Pulman, Stephen and Coecke, Bob},
  journal={Logic and Algebraic Structures in Quantum Computing},
  pages={199},
  year={2013}
}
                  

@article{berger2017cite2vec,
  title={cite2vec: citation-driven document exploration via word embeddings},
  author={Berger, Matthew and McDonough, Katherine and Seversky, Lee M},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={691--700},
  year={2017},
  publisher={IEEE}
}

@article{lagarias2002beyond,
  title={Beyond the Descartes circle theorem},
  author={Lagarias, Jeffrey C and Mallows, Colin L and Wilks, Allan R},
  journal={The American mathematical monthly},
  volume={109},
  number={4},
  pages={338--361},
  year={2002},
  publisher={JSTOR}
}

@article{prakash2018,
  author    = {Aaditya Prakash and
               Nick Moran and
               Solomon Garber and
               Antonella DiLillo and
               James A. Storer},
  title     = {Deflecting Adversarial Attacks with Pixel Deflection},
  journal   = {CoRR},
  volume    = {abs/1801.08926},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.08926},
  archivePrefix = {arXiv},
  eprint    = {1801.08926},
  timestamp = {Fri, 02 Feb 2018 14:20:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-08926},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gatys2015,
  author    = {Leon A. Gatys and
               Alexander S. Ecker and
               Matthias Bethge},
  title     = {A Neural Algorithm of Artistic Style},
  journal   = {CoRR},
  volume    = {abs/1508.06576},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.06576},
  archivePrefix = {arXiv},
  eprint    = {1508.06576},
  timestamp = {Wed, 07 Jun 2017 14:41:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GatysEB15a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Haber2017,
  author    = {Eldad Haber and
               Lars Ruthotto},
  title     = {Stable Architectures for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1705.03341},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03341},
  archivePrefix = {arXiv},
  eprint    = {1705.03341},
  timestamp = {Wed, 07 Jun 2017 14:40:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HaberR17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chang2017,
  author    = {Bo Chang and
               Lili Meng and
               Eldad Haber and
               Lars Ruthotto and
               David Begert and
               Elliot Holtham},
  title     = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1709.03698},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.03698},
  archivePrefix = {arXiv},
  eprint    = {1709.03698},
  timestamp = {Thu, 05 Oct 2017 09:42:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-03698},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Szegedy2013,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  title     = {Intriguing properties of neural networks},
  journal   = {CoRR},
  volume    = {abs/1312.6199},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.6199},
  archivePrefix = {arXiv},
  eprint    = {1312.6199},
  timestamp = {Wed, 07 Jun 2017 14:41:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyZSBEGF13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@article{Johnson2016,
  author    = {Justin Johnson and
               Alexandre Alahi and
               Fei{-}Fei Li},
  title     = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  journal   = {CoRR},
  volume    = {abs/1603.08155},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.08155},
  archivePrefix = {arXiv},
  eprint    = {1603.08155},
  timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JohnsonAL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
                  

@article{prakash_ecting_nodate,
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	abstract = {CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.},
	language = {en},
	author = {Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James and University, Brandeis},
	pages = {17},
	file = {Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\QUNFRPBY\\Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:application/pdf}
}

@article{haber_stable_2018,
	title = {Stable architectures for deep neural networks},
	volume = {34},
	issn = {0266-5611, 1361-6420},
	url = {http://stacks.iop.org/0266-5611/34/i=1/a=014004?key=crossref.1cc46f347b817746f33b5329460be31b},
	doi = {10.1088/1361-6420/aa9a90},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.},
	language = {en},
	number = {1},
	urldate = {2018-04-25},
	journal = {Inverse Problems},
	author = {Haber, Eldad and Ruthotto, Lars},
	month = {jan},
	year = {2018},
	pages = {014004},
	file = {Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\VGE5DB2I\\Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:application/pdf}
}

@article{gatys_neural_2016,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	volume = {16},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/16.12.326},
	doi = {10.1167/16.12.326},
	language = {en},
	number = {12},
	urldate = {2018-04-25},
	journal = {Journal of Vision},
	author = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
	month = {sep},
	year = {2016},
	pages = {326},
	file = {Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\8TSSA4PG\\Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:application/pdf}
}

@article{chang_reversible_nodate,
	title = {Reversible {Architectures} for {Arbitrarily} {Deep} {Residual} {Neural} {Networks}},
	abstract = {Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memoryefficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.},
	language = {en},
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	pages = {8},
	file = {Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\CS6F3WMH\\Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf}
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2018-04-25},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = {dec},
	year = {2013},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1312.6199 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\CGVM35NM\\Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\LNI6UKWR\\1312.html:text/html}
}

@article{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	urldate = {2018-04-25},
	journal = {arXiv:1603.08155 [cs]},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	month = {mar},
	year = {2016},
	note = {arXiv: 1603.08155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1603.08155 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\8P957BLX\\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\5XBNB97T\\1603.html:text/html}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2018-04-25},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = {dec},
	year = {2014},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\4AJ5ZRYV\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KB34UKC8\\1412.html:text/html}
}

@article{kurakin_adversarial_2016,
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
	urldate = {2018-04-25},
	journal = {arXiv:1607.02533 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = {jul},
	year = {2016},
	note = {arXiv: 1607.02533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ\_uMenoBCk},
	file = {arXiv\:1607.02533 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\35I7YL6G\\Kurakin et al. - 2016 - Adversarial examples in the physical world.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6BVECYWQ\\1607.html:text/html}
}

@article{papernot_cleverhans_2016,
	title = {cleverhans v2.0.0: an adversarial machine learning library},
	shorttitle = {cleverhans v2.0.0},
	url = {http://arxiv.org/abs/1610.00768},
	abstract = {{\textbackslash}texttt\{cleverhans\} is a software library that provides standardized reference implementations of {\textbackslash}emph\{adversarial example\} construction techniques and {\textbackslash}emph\{adversarial training\}. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section{\textasciitilde}{\textbackslash}ref\{sec:introduction\} provides an overview of adversarial examples in machine learning and of the {\textbackslash}texttt\{cleverhans\} software. Section{\textasciitilde}{\textbackslash}ref\{sec:core\} presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section{\textasciitilde}{\textbackslash}ref\{sec:benchmark\} describes how to report benchmark results using the library. Section{\textasciitilde}{\textbackslash}ref\{sec:version\} describes the versioning system.},
	urldate = {2018-04-25},
	journal = {arXiv:1610.00768 [cs, stat]},
	author = {Papernot, Nicolas and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Faghri, Fartash and Matyasko, Alexander and Hambardzumyan, Karen and Juang, Yi-Lin and Kurakin, Alexey and Sheatsley, Ryan and Garg, Abhibhav and Lin, Yen-Chen},
	month = {oct},
	year = {2016},
	note = {arXiv: 1610.00768},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: Technical report for https://github.com/openai/cleverhans},
	file = {arXiv\:1610.00768 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\RTY5GJJN\\Papernot et al. - 2016 - cleverhans v2.0.0 an adversarial machine learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\GZ9CJGY3\\1610.html:text/html}
}

@article{papernot_practical_2016,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	url = {http://arxiv.org/abs/1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	urldate = {2018-04-25},
	journal = {arXiv:1602.02697 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = {feb},
	year = {2016},
	note = {arXiv: 1602.02697},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning},
	annote = {Comment: Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE},
	file = {arXiv\:1602.02697 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\7E32ER7G\\Papernot et al. - 2016 - Practical Black-Box Attacks against Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VGUUWBMH\\1602.html:text/html}
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	urldate = {2018-04-25},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = {nov},
	year = {2015},
	note = {arXiv: 1511.07528},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the 1st IEEE European Symposium on Security \& Privacy, IEEE 2016. Saarbrucken, Germany},
	file = {arXiv\:1511.07528 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\SI9WW5F6\\Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VQ5FT6KW\\1511.html:text/html}
}

@article{moosavi-dezfooli_deepfool:_2015,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	url = {http://arxiv.org/abs/1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	urldate = {2018-04-25},
	journal = {arXiv:1511.04599 [cs]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = {nov},
	year = {2015},
	note = {arXiv: 1511.04599},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016},
	file = {arXiv\:1511.04599 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\R4FMVV3B\\Moosavi-Dezfooli et al. - 2015 - DeepFool a simple and accurate method to fool dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KZ58SXIR\\1511.html:text/html}
}

@article{carlini_towards_2016,
	title = {Towards Evaluating the Robustness of Neural Networks},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	urldate = {2018-04-25},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = {aug},
	year = 2016,
	note = {arXiv: 1608.04644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.04644 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\Q4T8UTKH\\Carlini and Wagner - 2016 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\PQ7PVTRJ\\1608.html:text/html}
}

@article{madry_towards_2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	urldate = {2018-04-25},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = {jun},
	year = {2017},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\QL6LA2DS\\Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6YSIR8L9\\1706.html:text/html}
}

@article{su_one_2017,
	title = {One pixel attack for fooling deep neural networks},
	url = {http://arxiv.org/abs/1710.08864},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97\% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47\% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.},
	urldate = {2018-04-25},
	journal = {arXiv:1710.08864 [cs, stat]},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = {oct},
	year = {2017},
	note = {arXiv: 1710.08864},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.08864 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\47WQZ5NK\\Su et al. - 2017 - One pixel attack for fooling deep neural networks.pdf:application/pdf}
}
                  
@book{Sen2017Learning,
author = {Sen, Suvrajeet and Deng, Yunxiao},
title = {Learning Enabled Optimization: Towards a Fusion of Statistical Learning and Stochastic Optimization},
publisher = {Humboldt-Universitat zu Berlin},
year = {2017},
doi = {http://dx.doi.org/10.18452/18087}
}
                  
@article{volodmymyr2016,
  author    = {Volodymyr Mnih and
               Adri{\`{a}} Puigdom{\`{e}}nech Badia and
               Mehdi Mirza and
               Alex Graves and
               Timothy P. Lillicrap and
               Tim Harley and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1602.01783},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.01783},
  archivePrefix = {arXiv},
  eprint    = {1602.01783},
  timestamp = {Wed, 07 Jun 2017 14:43:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihBMGLHSK16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@article{Poupon2004,
title = "Voronoi and Voronoi-related tessellations in studies of protein structure and interaction",
journal = "Current Opinion in Structural Biology",
volume = "14",
number = "2",
pages = "233 - 241",
year = "2004",
issn = "0959-440X",
doi = "https://doi.org/10.1016/j.sbi.2004.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S0959440X04000442",
author = "Anne Poupon"
}                  
                  

@article {Guo1997,
author = {Guo, Baining and Menon, Jai and Willette, Brian},
title = {Surface Reconstruction Using Alpha Shapes},
journal = {Computer Graphics Forum},
volume = {16},
number = {4},
publisher = {Blackwell Publishers},
issn = {1467-8659},
url = {http://dx.doi.org/10.1111/1467-8659.00178},
doi = {10.1111/1467-8659.00178},
pages = {177--190},
keywords = {Surface topology, alpha shapes, manifolds, surface fitting},
year = {1997},
}

@Article{Maus1984,
author="Maus, Arne",
title="Delaunay triangulation and the convex hull ofn points in expected linear time",
journal="BIT Numerical Mathematics",
year="1984",
month="Jun",
day="01",
volume="24",
number="2",
pages="151--163",
abstract="An algorithm is presented which produces a Delaunay triangulation ofn points in the Euclidean plane in expected linear time. The expected execution time is achieved when the data are (not too far from) uniformly distributed. A modification of the algorithm discussed in the appendix treats most of the non-uniform distributions. The basis of this algorithm is a geographical partitioning of the plane into boxes by the well-known Radix-sort algorithm. This partitioning is also used as a basis for a linear time algorithm for finding the convex hull ofn points in the Euclidean plane.",
issn="1572-9125",
doi="10.1007/BF01937482",
url="https://doi.org/10.1007/BF01937482"
}

@Article{Lee1980,
author="Lee, D. T.
and Schachter, B. J.",
title="Two algorithms for constructing a Delaunay triangulation",
journal="International Journal of Computer {\&} Information Sciences",
year="1980",
month="Jun",
day="01",
volume="9",
number="3",
pages="219--242",
abstract="This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.",
issn="1573-7640",
doi="10.1007/BF00977785",
url="https://doi.org/10.1007/BF00977785"
}

@ARTICLE{Edelsbrunner1983, 
author={H. Edelsbrunner and D. Kirkpatrick and R. Seidel}, 
journal={IEEE Transactions on Information Theory}, 
title={On the shape of a set of points in the plane}, 
year={1983}, 
volume={29}, 
number={4}, 
pages={551-559}, 
keywords={Geometry;Image analysis, shape;Image shape analysis}, 
doi={10.1109/TIT.1983.1056714}, 
ISSN={0018-9448}, 
month={July},}

                  % dynamical clustering paper
@article{Drieme2017,
  author    = {Anne Driemel and
               Francesco Silvestri},
  title     = {Locality-sensitive hashing of curves},
  journal   = {CoRR},
  volume    = {abs/1703.04040},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04040},
  archivePrefix = {arXiv},
  eprint    = {1703.04040},
  timestamp = {Tue, 18 Jul 2017 10:49:06 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/DriemelS17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Sankararaman2013,
 author = {Sankararaman, Swaminathan and Agarwal, Pankaj K. and M{\o}lhave, Thomas and Pan, Jiangwei and Boedihardjo, Arnold P.},
 title = {Model-driven Matching and Segmentation of Trajectories},
 booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
 series = {SIGSPATIAL'13},
 year = {2013},
 isbn = {978-1-4503-2521-9},
 location = {Orlando, Florida},
 pages = {234--243},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2525314.2525360},
 doi = {10.1145/2525314.2525360},
 acmid = {2525360},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPS trajectories, trajectory matching, trajectory segmentation},
} 


@ARTICLE{Mirzargar2014, 
author={M. Mirzargar and R. T. Whitaker and R. M. Kirby}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Curve Boxplot: Generalization of Boxplot for Ensembles of Curves}, 
year={2014}, 
volume={20}, 
number={12}, 
pages={2654-2663}, 
keywords={computational geometry;data visualisation;boundary values;boxplot generalization;computational scientists;curve boxplot;curve ensembles;data depth;descriptive statistics;nonparametric method;rendering strategies;simulation science;visualization community;visualization strategies;Computational modeling;Curve fitting;Data visualization;Robustness;Shape analysis;Statistical analysis;Uncertainty visualization;boxplots;data depth;ensemble visualization;functional data;nonparametric statistic;order statistics;parametric curves;0}, 
doi={10.1109/TVCG.2014.2346455}, 
ISSN={1077-2626}, 
month={Dec},}

@article{Raj2017,
title = "Path Boxplots: A Method for Characterizing Uncertainty in Path Ensembles on a Graph",
abstract = "Graphs are powerful and versatile data structures that can be used to represent a wide range of different types of information. In this article, we introduce a method to analyze and then visualize an important class of data described over a graph-namely, ensembles of paths. Analysis of such path ensembles is useful in a variety of applications, in diverse fields such as transportation, computer networks, and molecular dynamics. The proposed method generalizes the concept of band depth to an ensemble of paths on a graph, which provides a center-outward ordering on the paths. This ordering is, in turn, used to construct a generalization of the conventional boxplot or whisker plot, called a path boxplot, which applies to paths on a graph. The utility of path boxplot is demonstrated for several examples of path ensembles including paths defined over computer networks and roads. Supplementary materials for this article are available online.",
keywords = "Data depth, Descriptive statistics, Ensemble, Graph, Paths",
author = "Mukund Raj and Mahsa Mirzargar and Robert Ricci and Kirby, {Robert M.} and Whitaker, {Ross T.}",
year = "2017",
month = "4",
doi = "10.1080/10618600.2016.1209115",
volume = "26",
pages = "243--252",
journal = "Journal of Computational and Graphical Statistics",
issn = "1061-8600",
publisher = "American Statistical Association",
number = "2",
}
                  
@Inbook{Kharrat2008,
author="Kharrat, Ahmed
and Popa, Iulian Sandu
and Zeitouni, Karine
and Faiz, Sami",
nothing="Ruas, Anne
and Gold, Christopher",
title="Clustering Algorithm for Network Constraint Trajectories",
bookTitle="Headway in Spatial Data Handling: 13th International Symposium on Spatial Data Handling",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="631--647",
abstract="Spatial data mining is an active topic in spatial databases. This paper proposes a new clustering method for moving object trajectories databases. It applies specifically to trajectories that only lie on a predefined network. The proposed algorithm (NETSCAN) is inspired from the well-known density based algorithms. However, it takes advantage of the network constraint to estimate the object density. Indeed, NETSCAN first computes dense paths in the network based on the moving object count, then, it clusters the sub-trajectories which are similar to the dense paths. The user can adjust the clustering result by setting a density threshold for the dense paths, and a similarity threshold within the clusters. This paper describes the proposed method. An implementation is reported, along with experimental results that show the effectiveness of our approach and the flexibility allowed by the user parameters.",
isbn="978-3-540-68566-1",
doi="10.1007/978-3-540-68566-1_36",
url="https://doi.org/10.1007/978-3-540-68566-1_36"
}
                  
@article{Zheng2015,
 author = {Zheng, Yu},
 title = {Trajectory Data Mining: An Overview},
 journal = {ACM Trans. Intell. Syst. Technol.},
 issue_date = {May 2015},
 volume = {6},
 number = {3},
 month = {may},
 year = {2015},
 issn = {2157-6904},
 pages = {29:1--29:41},
 articleno = {29},
 numpages = {41},
 url = {http://doi.acm.org/10.1145/2743025},
 doi = {10.1145/2743025},
 acmid = {2743025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Spatiotemporal data mining, trajectory classification, trajectory compression, trajectory data mining, trajectory indexing and retrieval, trajectory outlier detection, trajectory pattern mining, trajectory uncertainty, urban computing},
} 

@article{Shah12017,
author = {Shah, Sohil Atul and Koltun, Vladlen}, 
title = {Robust continuous clustering},
volume = {114}, 
number = {37}, 
pages = {9814-9819}, 
year = {2017}, 
doi = {10.1073/pnas.1700770114}, 
abstract ={Clustering is a fundamental procedure in the analysis of scientific data. It is used ubiquitously across the sciences. Despite decades of research, existing clustering algorithms have limited effectiveness in high dimensions and often require tuning parameters for different domains and datasets. We present a clustering algorithm that achieves high accuracy across multiple domains and scales efficiently to high dimensions and large datasets. The presented algorithm optimizes a smooth continuous objective, which is based on robust statistics and allows heavily mixed clusters to be untangled. The continuous nature of the objective also allows clustering to be integrated as a module in end-to-end feature learning pipelines. We demonstrate this by extending the algorithm to perform joint clustering and dimensionality reduction by efficiently optimizing a continuous global objective. The presented approach is evaluated on large datasets of faces, hand-written digits, objects, newswire articles, sensor readings from the Space Shuttle, and protein expression levels. Our method achieves high accuracy across all datasets, outperforming the best prior algorithm by a factor of 3 in average rank.}, 
URL = {http://www.pnas.org/content/114/37/9814.abstract}, 
eprint = {http://www.pnas.org/content/114/37/9814.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@ARTICLE{Jaromczyk1982, 
author={J. W. Jaromczyk and G. T. Toussaint}, 
journal={Proceedings of the IEEE}, 
title={Relative neighborhood graphs and their relatives}, 
year=1992, 
volume=80, 
number=9, 
pages={1502-1517}, 
keywords={computational geometry;computer vision;pattern recognition;spatial data structures;visual databases;computational morphology;computer vision;databases;neighborhood graphs;pattern classification;spatial analysis;Application software;Bibliographies;Biology computing;Computational geometry;Computer applications;Computer science;Computer vision;Morphology;Pattern analysis;Shape}, 
doi={10.1109/5.163414}, 
ISSN={0018-9219}, 
month={Sep},}

@inproceedings{Chakrabarti2006,
  title={Evolutionary clustering},
  author={Chakrabarti, Deepayan and Kumar, Ravi and Tomkins, Andrew},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining}, 
  pages={554--560},
  year={2006},
  organization={ACM}
}

@inproceedings{Andrade2001,
  title={Good approximations for the relative neighbourhood graph.},
  author={Andrade, Diogo Vieira and de Figueiredo, Luiz Henrique},
  booktitle={CCCG},
  pages={25--28},
  year={2001}
}

@book{greene2003,
  title={Econometric Analysis},
  author={Greene, W.H.},
  isbn={9788177586848},
  url={https://books.google.com/books?id=njAcXDlR5U8C},
  year={2003},
  publisher={Pearson Education}
}
                  
@article{mcfadden1973,
  title={Conditional logit analysis of qualitative choice behavior},
  author={McFadden, Daniel and others},
  year={1973},
  journal={Frontiers in Econometrics},
  publisher={Institute of Urban and Regional Development, University of California}
}

@phdthesis{novelli2015,
  title={Detection and Measurement of Sales Cannibalization in Information Technology Markets},
  author={Novelli, Francesco},
  year={2015},
  school={Technische Universit{\"a}t}
}

@article{draganska2004,
  title={A likelihood approach to estimating market equilibrium models},
  author={Draganska, Michaela and Jain, Dipak},
  journal={Management Science},
  volume={50},
  number={5},
  pages={605--616},
  year={2004},
  publisher={INFORMS}
}

@MISC{shriver2015,
  title={A Structural Model of Channel Choice with Implications for Retail Entry},
  author={Shriver, Scott and Bollinger, Bryan},
  year=2015
}

@article{fisher2009,
  title={An algorithm and demand estimation procedure for retail assortment optimization},
  author={Fisher, Marshall L and Vaidyanathan, Ramnath},
  journal={Philadelphia: The Wharton School},
  year={2009}
}

@article{liu1990,
  title={On a notion of data depth based on random simplices},
  author={Liu, Regina Y and others},
  journal={The Annals of Statistics},
  volume={18},
  number={1},
  pages={405--414},
  year={1990},
  publisher={Institute of Mathematical Statistics}
}

@article{lopez2009,
  title={On the concept of depth for functional data},
  author={L{\'o}pez-Pintado, Sara and Romo, Juan},
  journal={Journal of the American Statistical Association},
  volume={104},
  number={486},
  pages={718--734},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{rousseeuw1996,
  title={Algorithm AS 307: Bivariate location depth},
  author={Rousseeuw, Peter J and Ruts, Ida},
  journal={Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume={45},
  number={4},
  pages={516--526},
  year={1996},
  publisher={JSTOR}
}                  
                  
@inproceedings{cheng2001,
  title={On algorithms for simplicial depth.},
  author={Cheng, Andrew Y and Ouyang, Ming},
  booktitle={CCCG},
  pages={53--56},
  year={2001}
}
                  
@article{krishnan2006,
  title={Statistical data depth and the graphics hardware},
  author={Krishnan, Suresh and Mustafa, Nabil H and Venkatasubramanian, Suresh},
  journal={DIMACS Series in Discrete Mathematics and Theoretical Computer Science},
  volume={72},
  pages={223},
  year={2006},
  publisher={AMERICAN MATHEMATICAL SOCIETY}
}

@article{Zasenko2016,
  author    = {Olga Zasenko and
               Tamon Stephen},
  title     = {Algorithms for Colourful Simplicial Depth and Medians in the Plane},
  journal   = {CoRR},
  volume    = {abs/1608.07348},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.07348},
  archivePrefix = {arXiv},
  eprint    = {1608.07348},
  timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/ZasenkoS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}                  
                  
@article{Linderman2017,
  author    = {George C. Linderman and
               Stefan Steinerberger},
  title     = {Clustering with t-SNE, provably},
  journal   = {CoRR},
  volume    = {abs/1706.02582},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02582},
  archivePrefix = {arXiv},
  eprint    = {1706.02582},
  timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/LindermanS17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{wattenberg2016how,
  author = {Wattenberg, Martin and Viegas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}

@INPROCEEDINGS{Lee_Verleysen2014, 
author={J. A. Lee and M. Verleysen}, 
booktitle={2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)}, 
title={Two key properties of dimensionality reduction methods}, 
year={2014}, 
volume={}, 
number={}, 
pages={163-170}, 
keywords={data reduction;data structures;neural nets;principal component analysis;DR;data representation;deep neural networks;dimensionality reduction;principal component analysis;Cost function;Covariance matrices;Force;Manifolds;Plastics;Principal component analysis;Vectors}, 
doi={10.1109/CIDM.2014.7008663}, 
ISSN={}, 
month={Dec},}

@inproceedings{simon1996,
  title={Anonymous communication and anonymous cash},
  author={Simon, Daniel R},
  booktitle={Annual International Cryptology Conference},
  pages={61--73},
  year={1996},
  organization={Springer}
}
@inproceedings{al2016,
  title={Categorical Compositional Cognition},
  author={Al-Mehairi, Yaared and Coecke, Bob and Lewis, Martha},
  booktitle={International Symposium on Quantum Interaction},
  pages={122--134},
  year={2016},
  organization={Springer}
}
                  
@article{kartsaklis2013,
  title={Reasoning about meaning in natural language with compact closed categories and frobenius algebras},
  author={Kartsaklis, Dimitri and Sadrzadeh, Mehrnoosh and Pulman, Stephen and Coecke, Bob},
  journal={Logic and Algebraic Structures in Quantum Computing},
  pages={199},
  year={2013}
}
                  

@article{berger2017cite2vec,
  title={cite2vec: citation-driven document exploration via word embeddings},
  author={Berger, Matthew and McDonough, Katherine and Seversky, Lee M},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={691--700},
  year={2017},
  publisher={IEEE}
}

@article{lagarias2002beyond,
  title={Beyond the Descartes circle theorem},
  author={Lagarias, Jeffrey C and Mallows, Colin L and Wilks, Allan R},
  journal={The American mathematical monthly},
  volume={109},
  number={4},
  pages={338--361},
  year={2002},
  publisher={JSTOR}
}

@article{prakash2018,
  author    = {Aaditya Prakash and
               Nick Moran and
               Solomon Garber and
               Antonella DiLillo and
               James A. Storer},
  title     = {Deflecting Adversarial Attacks with Pixel Deflection},
  journal   = {CoRR},
  volume    = {abs/1801.08926},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.08926},
  archivePrefix = {arXiv},
  eprint    = {1801.08926},
  timestamp = {Fri, 02 Feb 2018 14:20:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-08926},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gatys2015,
  author    = {Leon A. Gatys and
               Alexander S. Ecker and
               Matthias Bethge},
  title     = {A Neural Algorithm of Artistic Style},
  journal   = {CoRR},
  volume    = {abs/1508.06576},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.06576},
  archivePrefix = {arXiv},
  eprint    = {1508.06576},
  timestamp = {Wed, 07 Jun 2017 14:41:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GatysEB15a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Haber2017,
  author    = {Eldad Haber and
               Lars Ruthotto},
  title     = {Stable Architectures for Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1705.03341},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.03341},
  archivePrefix = {arXiv},
  eprint    = {1705.03341},
  timestamp = {Wed, 07 Jun 2017 14:40:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HaberR17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chang2017,
  author    = {Bo Chang and
               Lili Meng and
               Eldad Haber and
               Lars Ruthotto and
               David Begert and
               Elliot Holtham},
  title     = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1709.03698},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.03698},
  archivePrefix = {arXiv},
  eprint    = {1709.03698},
  timestamp = {Thu, 05 Oct 2017 09:42:58 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-03698},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Szegedy2013,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  title     = {Intriguing properties of neural networks},
  journal   = {CoRR},
  volume    = {abs/1312.6199},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.6199},
  archivePrefix = {arXiv},
  eprint    = {1312.6199},
  timestamp = {Wed, 07 Jun 2017 14:41:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SzegedyZSBEGF13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@article{Johnson2016,
  author    = {Justin Johnson and
               Alexandre Alahi and
               Fei{-}Fei Li},
  title     = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  journal   = {CoRR},
  volume    = {abs/1603.08155},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.08155},
  archivePrefix = {arXiv},
  eprint    = {1603.08155},
  timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/JohnsonAL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
                  

@article{prakash_ecting_nodate,
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	abstract = {CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.},
	language = {en},
	author = {Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James and University, Brandeis},
	pages = {17},
	file = {Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\QUNFRPBY\\Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:application/pdf}
}

@article{haber_stable_2018,
	title = {Stable architectures for deep neural networks},
	volume = {34},
	issn = {0266-5611, 1361-6420},
	url = {http://stacks.iop.org/0266-5611/34/i=1/a=014004?key=crossref.1cc46f347b817746f33b5329460be31b},
	doi = {10.1088/1361-6420/aa9a90},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.},
	language = {en},
	number = {1},
	urldate = {2018-04-25},
	journal = {Inverse Problems},
	author = {Haber, Eldad and Ruthotto, Lars},
	month = {jan},
	year = {2018},
	pages = {014004},
	file = {Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\VGE5DB2I\\Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:application/pdf}
}

@article{gatys_neural_2016,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	volume = {16},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/16.12.326},
	doi = {10.1167/16.12.326},
	language = {en},
	number = {12},
	urldate = {2018-04-25},
	journal = {Journal of Vision},
	author = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
	month = {sep},
	year = {2016},
	pages = {326},
	file = {Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:C\:\\Use'rs\\Nexus\\Zotero\\storage\\8TSSA4PG\\Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:application/pdf}
}

@article{chang_reversible_2017,
	title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks}, 
	abstract = {Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memoryefficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.},
	month = {Nov},
	year = {2017},
	language = {en},
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	pages = {8},
	file = {Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\CS6F3WMH\\Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf}
}

@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2018-04-25},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = {dec},
	year = {2013},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1312.6199 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\CGVM35NM\\Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\LNI6UKWR\\1312.html:text/html}
}

@article{johnson_perceptual_2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	url = {http://arxiv.org/abs/1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	urldate = {2018-04-25},
	journal = {arXiv:1603.08155 [cs]},
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	month = {mar},
	year = {2016},
	note = {arXiv: 1603.08155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1603.08155 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\8P957BLX\\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\5XBNB97T\\1603.html:text/html}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2018-04-25},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = {dec},
	year = {2014},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\4AJ5ZRYV\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KB34UKC8\\1412.html:text/html}
}

@article{kurakin_adversarial_2016,
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
	urldate = {2018-04-25},
	journal = {arXiv:1607.02533 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = {jul},
	year = {2016},
	note = {arXiv: 1607.02533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ\_uMenoBCk},
	file = {arXiv\:1607.02533 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\35I7YL6G\\Kurakin et al. - 2016 - Adversarial examples in the physical world.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6BVECYWQ\\1607.html:text/html}
}

@article{papernot_cleverhans_2016,
	title = {cleverhans v2.0.0: an adversarial machine learning library},
	shorttitle = {cleverhans v2.0.0},
	url = {http://arxiv.org/abs/1610.00768},
	abstract = {{\textbackslash}texttt\{cleverhans\} is a software library that provides standardized reference implementations of {\textbackslash}emph\{adversarial example\} construction techniques and {\textbackslash}emph\{adversarial training\}. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section{\textasciitilde}{\textbackslash}ref\{sec:introduction\} provides an overview of adversarial examples in machine learning and of the {\textbackslash}texttt\{cleverhans\} software. Section{\textasciitilde}{\textbackslash}ref\{sec:core\} presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section{\textasciitilde}{\textbackslash}ref\{sec:benchmark\} describes how to report benchmark results using the library. Section{\textasciitilde}{\textbackslash}ref\{sec:version\} describes the versioning system.},
	urldate = {2018-04-25},
	journal = {arXiv:1610.00768 [cs, stat]},
	author = {Papernot, Nicolas and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Faghri, Fartash and Matyasko, Alexander and Hambardzumyan, Karen and Juang, Yi-Lin and Kurakin, Alexey and Sheatsley, Ryan and Garg, Abhibhav and Lin, Yen-Chen},
	month = {oct},
	year = {2016},
	note = {arXiv: 1610.00768},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: Technical report for https://github.com/openai/cleverhans},
	file = {arXiv\:1610.00768 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\RTY5GJJN\\Papernot et al. - 2016 - cleverhans v2.0.0 an adversarial machine learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\GZ9CJGY3\\1610.html:text/html}
}

@article{papernot_practical_2016,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	url = {http://arxiv.org/abs/1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	urldate = {2018-04-25},
	journal = {arXiv:1602.02697 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = {feb},
	year = {2016},
	note = {arXiv: 1602.02697},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning},
	annote = {Comment: Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE},
	file = {arXiv\:1602.02697 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\7E32ER7G\\Papernot et al. - 2016 - Practical Black-Box Attacks against Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VGUUWBMH\\1602.html:text/html}
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	urldate = {2018-04-25},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = {nov},
	year = {2015},
	note = {arXiv: 1511.07528},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the 1st IEEE European Symposium on Security \& Privacy, IEEE 2016. Saarbrucken, Germany},
	file = {arXiv\:1511.07528 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\SI9WW5F6\\Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VQ5FT6KW\\1511.html:text/html}
}

@article{moosavi-dezfooli_deepfool:_2015,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	url = {http://arxiv.org/abs/1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	urldate = {2018-04-25},
	journal = {arXiv:1511.04599 [cs]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = {nov},
	year = {2015},
	note = {arXiv: 1511.04599},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016},
	file = {arXiv\:1511.04599 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\R4FMVV3B\\Moosavi-Dezfooli et al. - 2015 - DeepFool a simple and accurate method to fool dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KZ58SXIR\\1511.html:text/html}
}

@article{carlini_towards_2016,
	title = {Towards Evaluating the Robustness of Neural Networks},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	urldate = {2018-04-25},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = {aug},
	year = 2016,
	note = {arXiv: 1608.04644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.04644 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\Q4T8UTKH\\Carlini and Wagner - 2016 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\PQ7PVTRJ\\1608.html:text/html}
}

@article{madry_towards_2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	urldate = {2018-04-25},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = {jun},
	year = {2017},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\QL6LA2DS\\Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6YSIR8L9\\1706.html:text/html}
}

@article{su_one_2017,
	title = {One pixel attack for fooling deep neural networks},
	url = {http://arxiv.org/abs/1710.08864},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97\% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47\% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.},
	urldate = {2018-04-25},
	journal = {arXiv:1710.08864 [cs, stat]},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = {oct},
	year = {2017},
	note = {arXiv: 1710.08864},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.08864 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\47WQZ5NK\\Su et al. - 2017 - One pixel attack for fooling deep neural networks.pdf:application/pdf}
}
                  

@Misc{aksh00,
author =   {Akshay Chawla},
title =    {{GTS}: {GNU} {Triangulated} {Surface} library},
howpublished = {https://github.com/akshaychawla/Adversarial-Examples-in-PyTorch},
year = 2017,
month = {November}
}

                  
@book{Bishop:2006:PRM:1162264,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 
                  
@misc{arden2018,
	title = {Applied-{Deep}-{Learning}-with-{Keras}/{Part} 4 ({GPU}) - {Convolutional} {Neural} {Networks}.ipynb at master  ardendertat/{Applied}-{Deep}-{Learning}-with-{Keras}  {GitHub}},
	author={Arden Dertat},
	url = {https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%204%20%28GPU%29%20-%20Convolutional%20Neural%20Networks.ipynb},
	urldate = {2018-06-30},
	year = {2018}
}

@article{nair_rectified_nodate,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	author = {Nair, Vinod and Hinton, Geoffrey E},
   booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
	year = {2010},
	file = {Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\MEQHIX28\\Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf}
}

                  
@article{itkinaadversarial,
  title={Adversarial Attacks on Image Recognition},
  author={Itkina, Masha and Wu, Yu and Bahmani, Bahman}
}
@article{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  journal={arXiv preprint},
  year={2017},
  brianfield={craig, this is the text content}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
                  
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},
  pages={618--626},
  year={2017},
  organization={IEEE}
}
                  

@INPROCEEDINGS{subakan2018, 
author={Y. C. {Subakan} and P. {Smaragdis}}, 
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Generative Adversarial Source Separation}, 
year={2018}, 
volume={}, 
number={}, 
pages={26-30}, 
keywords={blind source separation;learning (artificial intelligence);matrix decomposition;multilayer perceptrons;probability;source separation;speech processing;NMF;output probability density;data distributions;parametric assumption;output density;speech source separation experiment;Wasserstein-GAN formulation;generative adversarial networks;GAN;generative adversarial source;nonnegative matrix factorization;generative source separation methods;variational auto-encoders;Gallium nitride;Source separation;Training;Computational modeling;Spectrogram;Generative adversarial networks;Maximum likelihood estimation;Generative Adversarial Networks;Source Separation;Generative Models}, 
doi={10.1109/ICASSP.2018.8461671}, 
ISSN={2379-190X}, 
month={April},}

@book{yu2013blind,
  title={Blind source separation: theory and applications},
  author={Yu, Xianchuan and Hu, Dan and Xu, Jindong},
  year={2013},
  publisher={John Wiley \& Sons}
}
